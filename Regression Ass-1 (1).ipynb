{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6351c8fb-4ef9-4bb2-a12a-adffa95f0d74",
   "metadata": {},
   "source": [
    "# Regression Assignment-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1255751d-04e4-4edf-8e5f-573e85e57b88",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0feb7b1-73cb-4cf7-bcae-0aa109b7180b",
   "metadata": {},
   "source": [
    "Simple linear Regression - Aims to find a linear relationship to describe the correlation between a independent variable and dependent variable.\n",
    "\n",
    "simple linear example - Let's consider a dataset that contains information about the relationship between the number of years of experience an employee has and their corresponding salary. The goal is to predict an employee's salary based solely on their years of experience.\n",
    "\n",
    "year of experience =[1,2,3,4,5] and a salary=[40000,50000,60000,75000,80000]\n",
    "\n",
    "Multiple linear Regression - (MLR) is used to determine a relationship among random variable.\n",
    "\n",
    "Multiple Linear Example - let's expend above example with some others variable like education and region (urban-1 and rural-0) categorise like this.\n",
    "\n",
    "year of experience =[1,2,3,4,5] , education = [12,13,14,,15,17] , region = [0,1,0,0,1] , salary=[40000,50000,60000,75000,80000] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af10ff-7a62-4935-b2f9-80a09833424e",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84545ed5-c5f6-4951-996a-19dcc6bf580b",
   "metadata": {},
   "source": [
    "Linearity: The relationship between the independent variables and the dependent variable should be linear. This means that changes in the independent variables should result in proportional changes in the dependent variable.\n",
    "\n",
    "Independence of errors: The errors (residuals) should be independent of each other. In other words, there should be no systematic patterns in the residuals. This assumption ensures that each data point provides new information and is not influenced by the other data points.\n",
    "\n",
    "Homoscedasticity: The variance of the errors should be constant across all levels of the independent variables. In practical terms, this means that the spread of the residuals should remain consistent as the values of the independent variables change.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, several diagnostic techniques can be employed:\n",
    "\n",
    "Residual plots: Plotting the residuals against the predicted values or against each independent variable can help identify patterns or trends in the residuals. A random scatter of points around zero suggests that the assumptions of linearity and homoscedasticity are met.\n",
    "\n",
    "Normality tests: Statistical tests, such as the Shapiro-Wilk test or visual inspection of a histogram or Q-Q plot of the residuals, can assess whether the residuals follow a normal distribution.\n",
    "\n",
    "Durbin-Watson test: This test assesses the independence of errors by detecting autocorrelation in the residuals. A value close to 2 indicates no significant autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2afbc2b-0643-4c8c-8f6d-bdf1ca8b33b8",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0165849-38a1-4844-90c2-0ec86fb7be93",
   "metadata": {},
   "source": [
    "Intercept (\n",
    "�\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    " ): The intercept represents the predicted value of the dependent variable when the independent variable is zero. In other words, it is the value of \n",
    "�\n",
    "Y when \n",
    "�\n",
    "X equals zero. The intercept captures the baseline level of the dependent variable when all predictors are absent or have a value of zero.\n",
    "\n",
    "Slope (\n",
    "�\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    " ): The slope coefficient represents the change in the dependent variable for a one-unit increase in the independent variable, holding all other variables constant. It indicates the rate of change in the dependent variable per unit change in the independent variable.\n",
    " \n",
    " Example:-\n",
    " \n",
    " Let's consider a real-world scenario where we want to predict the price of a house based on its size (in square feet). We collect data on house prices and their corresponding sizes and fit a linear regression model to the data. The regression equation might look like this:\n",
    " \n",
    "Intercept: Think of this as the starting point. It's like saying, \"Even if the size of the house is zero (which doesn't really happen), you'd still expect to pay $50,000 for it.\" It's the price you start with, regardless of the size.\n",
    "\n",
    "Slope: This tells you how much the price changes when you add one more square foot to the house. For example, if the slope is 100, it means that for every extra square foot, the price goes up by $100. So, if you add 10 more square feet, the price would increase by $1000. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20a5b5f-2cbe-4675-8348-652d633ff3be",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b79089e-95c9-447a-b951-4779164df523",
   "metadata": {},
   "source": [
    "Initialization: The algorithm starts by initializing the model parameters with random values or predefined values.\n",
    "\n",
    "Compute the Gradient: The gradient of the loss function with respect to each parameter is computed. The gradient indicates the direction of the steepest increase of the function.\n",
    "\n",
    "Update the Parameters: The parameters are updated by taking a small step in the direction opposite to the gradient. This step is determined by the learning rate, which controls the size of the step.\n",
    "\n",
    "Iterate: Steps 2 and 3 are repeated iteratively until a stopping criterion is met, such as reaching a maximum number of iterations or the change in the loss function becoming negligible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fd5a34-f226-4ff3-b96c-ba6d6c1068e2",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f856521-4968-474a-868e-f14a503c780d",
   "metadata": {},
   "source": [
    "Multiple linear regression is a statistical technique used to analyze the relationship between multiple independent variables and a single dependent variable. It extends the principles of simple linear regression, which involves only one independent variable.\n",
    "\n",
    "In multiple linear regression we have many independent variables whereas in simple linear regression we have only one independent variable and it's equation goes like Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X+ϵ  in that way multiple linear regression is differ from simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1cd13a-13e1-42a2-b7a0-3d21d72c267d",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97a5fed-be39-4332-8253-6847f7f5b268",
   "metadata": {},
   "source": [
    "Multicollinearity occurs in multiiple linear regression when two or more independent variable in model are highly correlated with each other.This can cause problems because it becomes difficult for the model to differentiate the effects of each individual predictor on the dependent variable.\n",
    "\n",
    "To detect multicollinearity..\n",
    "\n",
    "Correlation Matrix: Calculate the correlation coefficients between all pairs of independent variables. If you see coefficients close to 1 or -1, it indicates high correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784cbbbc-c692-4dcf-90da-7717fb6cd32e",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a66fbc-6b38-47bc-ba2a-5519aa6d290a",
   "metadata": {},
   "source": [
    "Polynomial regression is a type of regression analysis used when the relationship between the independent variable (or variables) and the dependent variable is nonlinear. It extends the principles of linear regression by fitting a polynomial equation to the data instead of a straight line.\n",
    "\n",
    "The main difference between polynomial regression and linear regression is the form of the relationship between the independent and dependent variables. In linear regression, the relationship is assumed to be linear, meaning it can be represented by a straight line. However, in polynomial regression, the relationship can take on a more curved shape, allowing for a better fit to nonlinear data patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cf4bc0-023a-4390-a7dd-ef846562736d",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c46a286-bfc8-4cb0-b035-0a26c772aa4d",
   "metadata": {},
   "source": [
    "Advantages:-\n",
    "\n",
    "Flexibility: Polynomial regression can capture nonlinear relationships between variables more effectively than linear regression. It can fit curves to data, allowing for a more flexible modeling approach.\n",
    "\n",
    "Improved Fit: By including higher-degree polynomial terms, polynomial regression can better fit complex data patterns that linear regression might not capture accurately.\n",
    "\n",
    "Disadvantages:-\n",
    "\n",
    "Overfitting: Using higher-degree polynomial terms can lead to overfitting, where the model fits the training data too closely and performs poorly on new, unseen data.\n",
    "\n",
    "Complexity: Polynomial regression models can become complex, especially when including higher-degree terms. This complexity makes interpretation more challenging and can lead to less intuitive models.\n",
    "\n",
    "Nonlinear Relationships: The relationship between the independent and dependent variables is nonlinear, and a straight line does not adequately represent the data pattern.\n",
    "\n",
    "Curved Patterns: The data exhibit curved patterns or have peaks and valleys that cannot be captured by a linear model.\n",
    "\n",
    "Improved Accuracy: When higher accuracy is required and sacrificing some interpretability is acceptable, polynomial regression might be suitable, especially if it provides a better fit to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713131b1-807e-4f28-9875-42fed5d47684",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
